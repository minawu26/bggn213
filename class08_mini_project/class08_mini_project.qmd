---
title: "class08_mini_project"
author: "A59013200 (Mina Wu)"
format: pdf
editor: visual
---

Unsupervised LEarning Analysis of Human Breast Cancer Cells.

Our data

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

> Q1. How many observations/samples/patients/rows?

There are `r nrow(wisc.df)` individuals in this dtaset.

> Q2. What is in the '\$diagnosis\` column? How many of each type?

```{r}
sum(wisc.df$diagnosis == "M")
sum(wisc.df$diagnosis =="B")
```

Use table.

```{r}
table(wisc.df$diagnosi)
```

> Q3. How many variables/features in the data are suffixed with \_mean?

```{r}
length(grep("_mean",colnames(wisc.df), value=TRUE ))
```

> Q. How many variables/dimensions we have?

```{r}
ncol(wisc.df)
```

Save the diagnosis for reference later

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
head(diagnosis)
```

and remove or exclude this (diagnosis) column from any of our analysis.

```{r}
wisc.data <- wisc.df[,-1]
print(head(wisc.data))

```

Let's try clustering this data:

Hierarchical Clustering with `hclust()`

```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```

# Pricipal Component Analysis

Let's try PCA on this data. Before doing any analysis like weshould check if our input data needs to be called first?

side-note: example with mtcars

```{r}
head(mtcars)
```

```{r}
apply(mtcars,2,mean)
```

```{r}
apply(mtcars,2,sd)
#the variance is high, so scaling it helps to bring the values together with less spread such as with log-transform. 
```

Let's try a PCA on this car dataset.

```{r}
pc<- prcomp(mtcars)
summary(pc)
biplot(pc)
```

Scale the mtcars.

```{r}
pc.scale<- prcomp(mtcars, scale= TRUE)
summary(pc.scale)
biplot(pc.scale)

```

We want to scale,because some data have different spreads, so we want to scale to reduce that. We can check the plot before scaling and after scaling to check if we need to do that. Scale helps to reduce the SD.

# Back to our cancer data set.

Do we need to scale this dataset? Yes, we do because the spread is very different.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

head(apply(wisc.data,2,sd))
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale= TRUE)
#wisc.pr

```

How well do the PCS capture the variance in the original data?

```{r}
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

PC1 captures 44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

PC3 \>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

PC7

our main PC score plot(aka PC plot, PC1 vs PC2, coordination plot)

```{r}
attributes(wisc.pr)
```

```{r}
biplot(wisc.pr)
#plot is hard to read
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot generated by the biplot is hard to read as the cells are too compact together.

we need to build our own plot here:

```{r}
#wisc.pr$x
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis,xlab= "PCA1", ylab= "PCA2")
```

> Q8. Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? The variance of the cells lie toward PCA1 (capture by PCA1)

```{r}
plot(wisc.pr$x[, 1],wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

Make a nice ggplot version

```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
ggplot(pc, aes(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis))+
  geom_point()+
  xlab( "PCA1")+
  ylab( "PCA2")
```

```{r}
v<-summary(wisc.pr)
v$importance[2,]
```

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

```

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr\$sdev\^2). Save the result as an object called pr.var.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

The elbow is around 0.1.

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Communicating PCA results: \> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr\$rotation\[,1\]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

-0.26085376

```{r}
head(wisc.pr$rotation)

```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist<- dist(data.scaled)

```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method ="complete")
```

```{r}
plot(wisc.hclust)
abline(h=20, col="red", lty=2)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

H=20

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=20)

```

We can use the table() function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

# Using different methods

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

The `ward.D2` gives me the better results, as I can distinct the number of clusters clearly.

## 4. Combining methods

Here we will use the results of PCA as the input to a clustering analysis. We stare with 3 PCs.

```{r}
wisc.pr.hclust<- hclust(dist(wisc.pr$x[,1:3]), method ="ward.D2")
plot(wisc.pr.hclust)
abline(h=80, col="red")
```

> Q13. How well does the newly created model with four clusters separate out the two diagnoses?

By grps 1 and 2.

```{r}
grps <- cutree(wisc.pr.hclust, h=80)
table(grps)
```

> Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km\$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

By grps 1 and 2 and diagnosis by B and M.

```{r}
table(grps, diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km\$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

# Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc

```

```{r}
g<- as.factor(grps)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?

Patient 2.

```{r}
library(bio3d)
inputfile <- "~/Documents/UCSD Grad class/BGGN 213/Find_a_Gene/FindaGene_CD81_Multiple_alignment.fst"
aln <- read.fasta( inputfile )
aln
sim <- conserv(aln)
plot(sim, typ="h", xlab="Alignment position", ylab="Conservation score")
inds <- order(sim, decreasing=TRUE)
head(sim[inds])
positions <- data.frame(pos=1:length(sim),
                        aa=aln$ali[1,],
                        score=sim)

head(positions)
head( positions[inds,] )
aa123(positions[inds,]$aa)[1:3]
library (pheatmap)
ide <- seqidentity(aln)
pheatmap((1-ide))

```

```{r}
consensus_seq <- consensus (aln)
identity_matrix <- seqidentity(aln)
max_identity_index <- which.max(apply(identity_matrix, 1, max))
selected_sequence <- aln[[max_identity_index]]
result <- blast.pdb (selected_sequence)
```
